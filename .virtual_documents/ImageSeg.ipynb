import numpy as np
#we import numpy here for it's mathematical functions on arrays and multidimensional arrays
#we mostly use it here to arrange the labels on the confusion matrix.
#numpy is also used for the manipulation of array and it has functions that can contribute with mathematical equations such as linear algebra.

import cv2
#cv2 is a python library used for image processing. It can perform functions like filtering, object recognition, etc..
#cv2 allows users to develop, read, write, and manipulate the images with ease. 
#This import supports real-time image processing for application with advanced visions. It also 
#supports machine learning models that is widely used in robotics.

import matplotlib.pyplot as plt
#matplotlib is mostly used for the creation and visualization of graphs.
#matplotlib is the standard for most graphing and plotting in python
#it works by creating figures and plots and graphs inside the figures which we can add data to.

import pandas as pd 
#Pandas is a python library that is mostly used for its ability to examine, clean, analyze, and modify data within the dataframe.
#this is an import for adding the dataframes we will manipulate and it also displays data in tabular form


img = cv2.imread("6p.jpg", cv2.IMREAD_COLOR)
#This line of code reads the image in color mode that has a default format of BGR(Blue, Green, Red)
#IMREAD_COLOR specifies that the image should be read in color mode. The Integer 1 can also
#be used instead of cv2.IMREAD_COLOR, both of them are the same.

img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
#The cvtColor() function converts an image from one color space to another and there are 150 color-space conversions available.
#In this case we converted the BGR to an RGB color space.
#Color would seem inverted/incorrect when visualizing with non-OpenCV if the conversion is not applied.


plt.title('Raw Image')
#This line of code adds a title on the uppermost part of the image
#We used this to label the image to avoid confusion in preprocessing.

plt.imshow(img)
#This line of code load and shows the image to the screen.
#We used this to confirm if the image exists and is loadable.


img_bnw = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
# There are 150 color-space conversions available, and the cvtColor() function can change an image's color space.
# In this instance, the BGR was changed to a grayscale color space.

plt.title('Black and White Image')
#This line of code adds a title on the uppermost part of the image
#We used this to label the image to avoid confusion in preprocessing.

plt.imshow(img_bnw, cmap="gray")
#We used this in order to confirm if the image is really converted into a gray color space.
#This line of code also loads and shows the image to the screen.


img_blur = cv2.GaussianBlur(img_bnw, (15, 15), 0)
#cv2.GaussianBlur is a method that is used to make an image smoother by reducing its noise and detail
#The parameters define how much blur does it have in the x and y direction. the '0' states that
#OpenCV would automaticlly caclulate the standart deviation based on its kernel size.

plt.title('Blurred Image')
#This line of code adds a title on the uppermost part of the image
#We used this to label the image to avoid confusion in preprocessing.

plt.imshow(img_blur, cmap = "gray")
#We used this to confirm if the Gaussian Blur is applied to the image. As we can see
#the image is blurred our resulting into a smoother texture.
#This line of code also loads and shows the image to the screen.


ret, thresh_otsu = cv2.threshold(img_blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
#Otsu's Tresholding separates the image into two classes which is Foreground and Background.
#It finds a point between dark and bright pixels and where is it clearest. 
#This line of code finds the best treshold value automatically by analyzing the histogram. 
#It minimizes the variance of each classes which results in an optimal image segmentation.

plt.title('Otsu\'s Tresholding')
#This line of code adds a title on the uppermost part of the image
#We used this to label the image to avoid confusion in preprocessing.

plt.imshow(thresh_otsu, cmap = "gray")
#We used this in order to confirm if the Otsu Tresholding is applied. As we can see in the image below, 
#it looks black and white which is the binary version of a grayscale image.
#We can also see that it separates the image into two parts being foreground and background.
#This line of code also loads and shows the image to the screen.


thresh_adaptive = cv2.adaptiveThreshold(img_blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,9,2)
#This line of code a pplies the adaptive tresholding function using te Gaussian model to convert 
#a blurred image that is also in grayscale into a binary image.
#The reason we ned this is to fix Uneven Lighting, and improvements for feature detection, etc...

plt.title('Adaptive Treshold')
#This line of code adds a title on the uppermost part of the image
#We used this to label the image to avoid confusion in preprocessing.

plt.imshow(thresh_adaptive, cmap = "gray")
#We used this to confirm if the Adaptive Treshold function is applied. As we can see in the image below,
#Sharp edges and features are highlighted even in areas where there is uneven lighting..
#This line of code also loads and shows the image to the screen.


edges = cv2.Canny(img_blur,5, 30)
#This line of codee detects the edges of an image. It does it by the sharp changes in intensity such as outlines.
#This fins the edges in te blurred image which would identify and isolate the shapes for further processing.

plt.title('Edge Detection (Canny)')
plt.imshow(edges, cmap = "gray")


img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)

H_lower, H_higher = 20, 30
S_lower, S_higher = 48, 255
V_lower, V_higher = 50, 255

lower = np.array([H_lower, S_lower, V_lower])
higher = np.array([H_higher, S_higher, V_higher])

mask = cv2.inRange(img_hsv, lower, higher)

img_highlight = cv2.bitwise_and(img, img, mask = mask)

fig, ax = plt.subplots(1, 2)

ax[0].set_title('Mask')
ax[1].set_title('Mask Overlayed Over Original Image')
plt.tight_layout()

ax[0].imshow(mask, cmap='gray', vmin=0, vmax=255)
ax[1].imshow(img_highlight)
plt.show()


kernel = np.ones((5,5), np.uint8)
dilation = cv2.dilate(img_blur,kernel,iterations = 1)
erosion = cv2.erode(img_blur,kernel,iterations = 1)


fig, ax = plt.subplots(1, 3)

ax[0].set_title('Gaussian Blurr')
ax[1].set_title('Dilation')
ax[2].set_title('Errosion')
    
ax[0].imshow(img_blur, cmap='gray')
ax[1].imshow(dilation, cmap = "gray")
ax[2].imshow(erosion, cmap = "gray")
plt.show()


params = cv2.SimpleBlobDetector_Params()
#To start using CV2 for detecting blobs, we first need to create a Simple Blob Detector Parameter Object.
#The parameter object contains all the configurations and option that we will use for the detector itself

image_to_blob = thresh_otsu.copy()
#We then specify an image to use for the blob detection and copy that.
#We will use the Otsu's Method image because out of all the binary images, it has the more distinct shapes

params.filterByArea = True
#Here we enable filtering the blobs by their area in pixels
#This helps remove any blobs too small that might just be noise
params.minArea = 100
#Here we set the minimum area of the blobs in pixels.
#With the size of the image of around 700 x 1000 pixels, a 100 pixel area is a reasonably
#small enough size for relevant shapes
params.filterByColor = True
#Here we set filtering blobs based on their color, specifically their intensity in the grayscale
#We need this option on because this is a binary image, we need to distinguish white blobs from
#black blobs
params.blobColor = 255
#We set the blob color filttr to only consider white blobs.
#This is because in the Otsu image we got, most of the distinct shapes are white
params.filterByCircularity = False
#We disable the circularity filtering here so we can detect blobs of any shape
#Circularity measures how close the blob is from being a perfect circle
params.filterByConvexity = False
#We disable convexity filtering here so we can allow blobs with cave ins and indents on the edges
#Convexity measures the blobs contour to the convex shape that holds it (it's convex hull)
params.filterByInertia = False
#We disable filtering by inertia, which indicates its elongation
#The inertia ratio measures how stretched or how linear the blob is

detector = cv2.SimpleBlobDetector_create(params)
#The detector is created here using the parameters we just set up
#This is the actual object that will scan the image for the blobs

keypoints = detector.detect(image_to_blob)
#Here we detect the blobs from the image and store them in an list of keypoints
#Each of the keypoints is an object containing all data of the keypoint, important to us is the
#coordinate and the size
img_with_keypoints = cv2.drawKeypoints(image_to_blob, keypoints, np.array([]), (0, 0, 255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
#Here we draw the otsu image and overlay it with the keypoints using blue circles
#This is useful so we can visually verify where the blobs are in the image

plt.title('Blob Detection')
# Before we actually display the image itself we set its title.
# We set the title of the Image using matplotlib here.

plt.imshow(img_with_keypoints, cmap = "gray")
#We display the image on this line using the imshow function
#Because this is a binary image, we use cmap gray to accurately display it
#given the data in the image object

keypoint_data = []
#We initialize an array to store data about the keypoints that we need
#This will be used to display it as a table as using the keypoints aren't enough
#because they are python objects

for keypoint in keypoints:
#We loop through every detected blob here
#Each of the keypoints (or blob) holds all the data, we will filter out what we need
    data = {}
    #We create an empty dictionary for data here
    #We will store both the coordinates and size here
    data["Coorrdinates"] = keypoint.pt
    #This gets the (x,y) corrdinates of the blob as a tuple.
    #We then store it in the data dictionary in the Coordinates key
    data["Size"] = keypoint.size
    #This gets size of the blob in terms of its diameter in pixels.
    #We then store it in the data dictionary in the Size key
    keypoint_data.append(data)
    #We then add it to the keypoint data list we made earlier.
    #After the loop, we will have a list of all the keypoints' size and coordinates

keypoint_data = pd.DataFrame(keypoint_data)
#We convert the list into a pandas Dataframe in this line
#We do that so it can be viewed as a table.
keypoint_data
#We display the data frame in this line
#As you can see when this is displayed, it is in tabular form and properly formatted


img_box = img.copy()
#We create a copy of the original image here.
#We only use this so we have something we can overlay on in the final output

new_img = img_blur.copy()
#The blurred image is copied in this line
#This is the image we will actually be manipulating
ret, new_otsu = cv2.threshold(new_img, 0, 255, cv2.THRESH_OTSU)
#We apply Otsu' thresholding on the blurred image
#We use this method as it's shapes more describe the general shapes of the 
#larger shapes in the image

contours, hierarchy = cv2.findContours(new_otsu, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
#We then find the contours in the Thresholded image
#The Contours are all the closed shapes it can detect in the image
#The hierarchy is the relationship of the contours in relation to each other

AREA_THRESH = 1500
#We then filter out the contours based on their size
#After this small contours will be filtered out leaving only larger more 
#relevant shapes

features = []
#We then initialize an empty array for the features
#Here we will store each of the information we need from each of the contours

for contour in contours:
#We start iterating through all the contours here.
#The contour itself are list of points outlining the shape
    area = cv2.contourArea(contour)
    #Here we get the size of the area enclosed by the contour
    #We use this to filter out small contours
    if area > AREA_THRESH:
    #Here we use the AREA THRESH variable we made earlier to filter
    #out smaller contours
    #From here on out, we will only be processing relatively large shapes
        moment = cv2.moments(contour)
        #Here we get the moment of the contour.
        #Image Moments are average values from the single pixels' intensities of an image
        #We mostly use moments to derive other values of a shape
        center_x, center_y = int(moment['m10']/moment['m00']), int(moment['m01']/moment['m00'])
        #Here we use the moment to calculate the center or centroid of the shape
        #This is how we derive values like these using the moments, using equations 
        #using the averages denoted by Mxx
        
        cv2.circle(img_box, (center_x, center_y), 7, (0, 0, 255), -1)
        #We draw a red dot in the center of the shape here.
        #We use the coordinates derived from the previous line and draw the dot there
        
        rect_x, rect_y, rect_w, rect_h = cv2.boundingRect(contour)
        #Here we get the coordinates of the bounding rectangle.
        #The bounding rectangle is axis aligned meaning its always a non-rotated rectangle
        min_rect = cv2.minAreaRect(contour)
        #Here we get the smallest rectangle that can bound the shape
        #This rectangle, unlike the previous one CAN be rotated
        img_box = cv2.rectangle(img_box, (rect_x, rect_y), (rect_x + rect_w, rect_y + rect_h), (0, 255, 0), 2)
        #Here we draw the larger rectanle as a Green Box
        #We plug in the coordinates of the upper left point, and the bottom right as well as the thickness as the parameters
        min_box = cv2.boxPoints(min_rect)
        #We then convert the rotated rectangle in the an object containing its corners
        #The new min_box object is the set of 4 points of the rectangle
        min_box = min_box.astype(np.intp)
        #The data in min_box is floating point, but we need it as an integer.
        #So we convert it first to an integer using numpy.
        cv2.drawContours(img_box, [min_box], 0, (0,0,255), 2)
        #We then draw the rotated rectangle onto the image.
        #Now we have both a green rectangle and a blue rectangle representing the object we detected
        
        feature = {}
        #Now we store the features we can derive from the contour.
        #We first initialize a dictionary to store the feature data
        feature["Perimeter"] = cv2.arcLength(contour, True)
        #The prerimeter of the object is stored using the arc length function
        #This is helpful in identifying the shapes and detecting any irregular outlines
        feature["Aspect Ratio"] = rect_w/rect_h
        #The aspect ratio is calculated using the original unrotated rectangle
        #It is calculated using the ratio of the width and the height of the rectangle
        feature["Extent"] = float(area) / (rect_w * rect_h)
        #The extent is the ratio of the contour's area with the are of the bounding box
        #It's essentially a measure of how much the contour actually fills the bounding box
        hull = cv2.convexHull(contour)
        #We then get the convex hull of the shape
        #The convex hull is the smallest shape that is rounded outward that can fill the shape
        hull_area = cv2.contourArea(hull)
        #Right now we only have a set of points of the convex hull not the area.
        #So we use the same function the calculate contour area to calculate the hull area
        feature["Solidity"] = float(area) / hull_area if hull_area > 0 else 0
        #We then calculate the solidity using the ratio of the contour area with the hull area
        #Similar to earlier this is basically how much the contour fills the hull
        feature["Diameter"] = np.sqrt(4 * area / np.pi).item()
        #We then calculate the diameter of the shape using 
        
        angle = None
        if len(contour) >= 5:
            ellipse = cv2.fitEllipse(contour)
            angle = ellipse[2]
        feature["Orientation"] = angle

        features.append(feature)
features = pd.DataFrame(features)

plt.title('Shape Detection Using Contours')
plt.imshow(img_box)
features



